{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def getNewLineIndices(text:str) -> np.array:\n",
    "    i = 0 # assume first char always opens paragraph\n",
    "    paragraph_indices = []\n",
    "    while i != -1:\n",
    "        paragraph_indices.append(i)\n",
    "        i = text.find('\\n', i + 1)\n",
    "    \n",
    "    return np.array(paragraph_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse data from \".ann\" UKP 2.0 files\n",
    "def readAnnotatedFile(ann_path:str) -> (dict, dict, dict, list, list):\n",
    "    propositions, prop_labels, prop_stances, supports, attacks = {}, {}, {}, [], []\n",
    "    with open(file=ann_path, mode='rt', encoding='utf8') as f:\n",
    "        for line in f:\n",
    "            delimited = line.split('\\t')\n",
    "            typ = delimited[0][0] # T == proposition , A = Stance, R = link\n",
    "            inner_index = int(delimited[0][1:])\n",
    "            data = delimited[1].split()\n",
    "            \n",
    "            if typ == 'T':\n",
    "                label = data[0] # prop lable (Premise, Cliam or MajorClaim)\n",
    "                start, end = int(data[1]), int(data[2]) # proposition offsets\n",
    "                propositions[inner_index] = (start, end) # represent propositions by it's index boundries\n",
    "                prop_labels[inner_index] = label\n",
    "                \n",
    "            elif typ == 'A':\n",
    "                _, target_index, stance_value = data # first Column in \"A\" lines is always \"Stance\", stance value in {For, Against}\n",
    "                prop_stances[int(target_index[1:])] = stance_value\n",
    "                \n",
    "            elif typ == 'R':\n",
    "                link_typ = data[0] # link type in {supports, attacks}\n",
    "                source, target = int(data[1][6:]), int(data[2][6:]) #get inner indices of related propositions (ex:Arg1:T4 Arg2:T3 -> source == 4 , target = 3)\n",
    "                link_list = supports if link_typ == 'supports' else attacks\n",
    "                link_list.append((source,target))\n",
    "    \n",
    "    return propositions, prop_labels, prop_stances, supports, attacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArgDoc(object):\n",
    "    def __init__(self, base_path):\n",
    "        self.ess_id = int(base_path[-3:]) # essay id according to UKP naming convention\n",
    "        self._txt_path = base_path + \".txt\" # essay text file path\n",
    "        self._ann_path = base_path + \".ann\" # UKP annotated file path\n",
    "        # read document's text\n",
    "        with open(file=self._txt_path, mode='rt', encoding='utf8') as f:\n",
    "            self.text = f.read()\n",
    "        \n",
    "        # get essay's paragraph's indices (seperated with '\\n')\n",
    "        self.paragraph_offsets = getNewLineIndices(self.text)\n",
    "        \n",
    "        # read annotated data from file\n",
    "        propositions, prop_labels, prop_stances, supports, attacks = readAnnotatedFile(self._ann_path)\n",
    "        \n",
    "        # update proposition offsets, labels, stances and link types\n",
    "        inner_indices, self.prop_offsets = zip(*sorted(propositions.items(), key = lambda x: x[1])) # use the beginning index of propositions for sort\n",
    "       \n",
    "        # paragraph alignmnt of propositions (ordered by proposition's offsets)\n",
    "        self.prop_paragraphs = [np.searchsorted(self.paragraph_offsets, start) -1 for start, _ in self.prop_offsets]\n",
    "        \n",
    "        # invert indices for key management \n",
    "        new_indices = {k: v for v, k in enumerate(inner_indices)}\n",
    "        n_props = len(self.prop_offsets)\n",
    "        \n",
    "        # update fields with new inverted indices\n",
    "        self.prop_labels = [prop_labels[inner_indices[i]] for i in range(n_props)]\n",
    "        self.prop_stances = {new_indices[k]: v for k,v in prop_stances.items()}\n",
    "        self.supports = [(new_indices[src], new_indices[trg]) for src, trg in supports]\n",
    "        self.attacks = [(new_indices[src], new_indices[trg]) for src, trg in attacks]\n",
    "        self.links = self.supports + self.attacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArgMLModel(object):\n",
    "    def __init__():\n",
    "        # TODO\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def visualizeUKPArgDoc(doc:ArgDoc):\n",
    "    \"\"\"\n",
    "    visualise UKP argument essay object and save to png\n",
    "    \"\"\"\n",
    "    import pydot\n",
    "    arg_graph = pydot.Dot(graph_type='digraph')\n",
    "    \n",
    "    maj_claims = [(\"! \" + doc.text[doc.prop_offsets[i][0]:doc.prop_offsets[i][1]]) for i in range(len(doc.prop_labels)) if doc.prop_labels[i] == 'MajorClaim'] # handle more than 1 major cliam for main node\n",
    "    # add the major claims node\n",
    "    head_node = pydot.Node('\\n'.join(maj_claims),style='filled',\n",
    "                           fillcolor = '#eeccdd')\n",
    "    arg_graph.add_node(head_node)\n",
    "    \n",
    "    nodes = {}\n",
    "    # add the premise and claims nodes\n",
    "    for i in range(len(doc.prop_labels)):\n",
    "        if doc.prop_labels[i] == 'MajorClaim':\n",
    "            continue\n",
    "        text = doc.text[doc.prop_offsets[i][0]:doc.prop_offsets[i][1]]\n",
    "        start = 0\n",
    "        label = []\n",
    "        next_i = -1\n",
    "        for i_c in range(1,len(text)):\n",
    "            if i_c < next_i:\n",
    "                continue                \n",
    "            if i_c % 30 == 0:\n",
    "                next_i = text.find(\" \",i_c) + 1\n",
    "                if next_i > 0:\n",
    "                    label.append(text[start:next_i - 1])\n",
    "                    start = next_i\n",
    "                else:\n",
    "                    label.append(text[start:])\n",
    "                    start = len(text)\n",
    "        if (i_c > start):\n",
    "            rest = \" \" + text[start:]\n",
    "            if len(rest.split()) == 1:\n",
    "                label[-1] += rest\n",
    "            else:\n",
    "                label.append(text[start:])\n",
    "\n",
    "        nodes[i] = pydot.Node(i,\n",
    "                              label = '\\n'.join(label),\n",
    "                              style ='filled',\n",
    "                              fillcolor = '#ccbbdd' if doc.prop_labels[i] == 'Claim' else '#aabbdd'\n",
    "                             )\n",
    "        arg_graph.add_node(nodes[i])\n",
    "        \n",
    "    # add edges\n",
    "    # add the stances (cliams-majorClaims) edges\n",
    "    for i,val in doc.prop_stances.items():\n",
    "        tmp_edge = pydot.Edge(nodes[i], head_node,\n",
    "                              label=val,\n",
    "                              labelfontcolor='red' if val == \"Against\" else 'green',\n",
    "                              color = 'red' if val == \"Against\" else 'green'\n",
    "                             )\n",
    "        arg_graph.add_edge(tmp_edge)\n",
    "    \n",
    "    # add the support/attacks edges\n",
    "    for src,trg in doc.supports:\n",
    "        tmp_edge = pydot.Edge(nodes[src],nodes[trg])\n",
    "        arg_graph.add_edge(tmp_edge)\n",
    "    \n",
    "    for src, trg in doc.attacks:\n",
    "        tmp_edge = pydot.Edge(nodes[src],nodes[trg],\n",
    "                              style = 'dotted',\n",
    "                              color = 'red'\n",
    "                             )\n",
    "        arg_graph.add_edge(tmp_edge)\n",
    "    \n",
    "    # display and save\n",
    "    path = os.path.join(os.getcwd(),\"essay{}.png\".format(doc.ess_id))\n",
    "    arg_graph.write_png(path)\n",
    "    print (\"saved png to {}\".format(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "arg_doc_ex = ArgDoc(base_path='/home/yochay/arg_mining_ukp/data/essay095')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizeUKPArgDoc(arg_doc_ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved png to /home/yochay/arg_mining_ukp/essay266.png\n",
      "saved png to /home/yochay/arg_mining_ukp/essay355.png\n",
      "saved png to /home/yochay/arg_mining_ukp/essay369.png\n",
      "saved png to /home/yochay/arg_mining_ukp/essay138.png\n",
      "saved png to /home/yochay/arg_mining_ukp/essay58.png\n",
      "saved png to /home/yochay/arg_mining_ukp/essay312.png\n",
      "saved png to /home/yochay/arg_mining_ukp/essay169.png\n",
      "saved png to /home/yochay/arg_mining_ukp/essay150.png\n",
      "saved png to /home/yochay/arg_mining_ukp/essay195.png\n",
      "saved png to /home/yochay/arg_mining_ukp/essay185.png\n",
      "saved png to /home/yochay/arg_mining_ukp/essay201.png\n",
      "saved png to /home/yochay/arg_mining_ukp/essay305.png\n",
      "saved png to /home/yochay/arg_mining_ukp/essay293.png\n",
      "saved png to /home/yochay/arg_mining_ukp/essay75.png\n",
      "saved png to /home/yochay/arg_mining_ukp/essay223.png\n",
      "saved png to /home/yochay/arg_mining_ukp/essay329.png\n",
      "saved png to /home/yochay/arg_mining_ukp/essay147.png\n",
      "saved png to /home/yochay/arg_mining_ukp/essay346.png\n",
      "saved png to /home/yochay/arg_mining_ukp/essay13.png\n",
      "saved png to /home/yochay/arg_mining_ukp/essay292.png\n",
      "saved png to /home/yochay/arg_mining_ukp/essay102.png\n",
      "saved png to /home/yochay/arg_mining_ukp/essay257.png\n",
      "saved png to /home/yochay/arg_mining_ukp/essay120.png\n",
      "saved png to /home/yochay/arg_mining_ukp/essay371.png\n",
      "saved png to /home/yochay/arg_mining_ukp/essay152.png\n",
      "saved png to /home/yochay/arg_mining_ukp/essay144.png\n",
      "saved png to /home/yochay/arg_mining_ukp/essay97.png\n",
      "saved png to /home/yochay/arg_mining_ukp/essay325.png\n",
      "saved png to /home/yochay/arg_mining_ukp/essay348.png\n",
      "saved png to /home/yochay/arg_mining_ukp/essay131.png\n",
      "saved png to /home/yochay/arg_mining_ukp/essay148.png\n",
      "saved png to /home/yochay/arg_mining_ukp/essay256.png\n",
      "saved png to /home/yochay/arg_mining_ukp/essay314.png\n",
      "saved png to /home/yochay/arg_mining_ukp/essay104.png\n",
      "saved png to /home/yochay/arg_mining_ukp/essay161.png\n",
      "saved png to /home/yochay/arg_mining_ukp/essay66.png\n",
      "saved png to /home/yochay/arg_mining_ukp/essay252.png\n",
      "saved png to /home/yochay/arg_mining_ukp/essay8.png\n",
      "saved png to /home/yochay/arg_mining_ukp/essay342.png\n",
      "saved png to /home/yochay/arg_mining_ukp/essay127.png\n",
      "saved png to /home/yochay/arg_mining_ukp/essay332.png\n",
      "saved png to /home/yochay/arg_mining_ukp/essay254.png\n",
      "saved png to /home/yochay/arg_mining_ukp/essay319.png\n",
      "saved png to /home/yochay/arg_mining_ukp/essay173.png\n",
      "saved png to /home/yochay/arg_mining_ukp/essay263.png\n",
      "saved png to /home/yochay/arg_mining_ukp/essay395.png\n",
      "saved png to /home/yochay/arg_mining_ukp/essay211.png\n",
      "saved png to /home/yochay/arg_mining_ukp/essay351.png\n",
      "saved png to /home/yochay/arg_mining_ukp/essay341.png\n",
      "saved png to /home/yochay/arg_mining_ukp/essay96.png\n",
      "saved png to /home/yochay/arg_mining_ukp/essay47.png\n",
      "saved png to /home/yochay/arg_mining_ukp/essay63.png\n",
      "saved png to /home/yochay/arg_mining_ukp/essay21.png\n",
      "saved png to /home/yochay/arg_mining_ukp/essay377.png\n",
      "saved png to /home/yochay/arg_mining_ukp/essay299.png\n",
      "saved png to /home/yochay/arg_mining_ukp/essay141.png\n",
      "saved png to /home/yochay/arg_mining_ukp/essay56.png\n",
      "saved png to /home/yochay/arg_mining_ukp/essay55.png\n",
      "saved png to /home/yochay/arg_mining_ukp/essay247.png\n",
      "saved png to /home/yochay/arg_mining_ukp/essay227.png\n",
      "saved png to /home/yochay/arg_mining_ukp/essay215.png\n",
      "saved png to /home/yochay/arg_mining_ukp/essay5.png\n",
      "saved png to /home/yochay/arg_mining_ukp/essay311.png\n",
      "saved png to /home/yochay/arg_mining_ukp/essay296.png\n",
      "saved png to /home/yochay/arg_mining_ukp/essay118.png\n",
      "saved png to /home/yochay/arg_mining_ukp/essay356.png\n",
      "saved png to /home/yochay/arg_mining_ukp/essay255.png\n",
      "saved png to /home/yochay/arg_mining_ukp/essay119.png\n",
      "saved png to /home/yochay/arg_mining_ukp/essay270.png\n",
      "saved png to /home/yochay/arg_mining_ukp/essay60.png\n",
      "saved png to /home/yochay/arg_mining_ukp/essay186.png\n",
      "saved png to /home/yochay/arg_mining_ukp/essay184.png\n",
      "saved png to /home/yochay/arg_mining_ukp/essay298.png\n",
      "saved png to /home/yochay/arg_mining_ukp/essay16.png\n",
      "saved png to /home/yochay/arg_mining_ukp/essay338.png\n",
      "saved png to /home/yochay/arg_mining_ukp/essay192.png\n",
      "saved png to /home/yochay/arg_mining_ukp/essay301.png\n",
      "saved png to /home/yochay/arg_mining_ukp/essay142.png\n",
      "saved png to /home/yochay/arg_mining_ukp/essay249.png\n",
      "saved png to /home/yochay/arg_mining_ukp/essay228.png\n",
      "saved png to /home/yochay/arg_mining_ukp/essay54.png\n",
      "saved png to /home/yochay/arg_mining_ukp/essay62.png\n",
      "saved png to /home/yochay/arg_mining_ukp/essay339.png\n",
      "saved png to /home/yochay/arg_mining_ukp/essay241.png\n",
      "saved png to /home/yochay/arg_mining_ukp/essay284.png\n",
      "saved png to /home/yochay/arg_mining_ukp/essay328.png\n",
      "saved png to /home/yochay/arg_mining_ukp/essay143.png\n",
      "saved png to /home/yochay/arg_mining_ukp/essay313.png\n",
      "saved png to /home/yochay/arg_mining_ukp/essay213.png\n",
      "saved png to /home/yochay/arg_mining_ukp/essay81.png\n",
      "saved png to /home/yochay/arg_mining_ukp/essay98.png\n",
      "saved png to /home/yochay/arg_mining_ukp/essay297.png\n",
      "saved png to /home/yochay/arg_mining_ukp/essay308.png\n",
      "saved png to /home/yochay/arg_mining_ukp/essay248.png\n",
      "saved png to /home/yochay/arg_mining_ukp/essay34.png\n",
      "saved png to /home/yochay/arg_mining_ukp/essay51.png\n",
      "saved png to /home/yochay/arg_mining_ukp/essay35.png\n",
      "saved png to /home/yochay/arg_mining_ukp/essay85.png\n",
      "saved png to /home/yochay/arg_mining_ukp/essay109.png\n",
      "saved png to /home/yochay/arg_mining_ukp/essay67.png\n",
      "saved png to /home/yochay/arg_mining_ukp/essay357.png\n",
      "saved png to /home/yochay/arg_mining_ukp/essay36.png\n",
      "saved png to /home/yochay/arg_mining_ukp/essay71.png\n",
      "saved png to /home/yochay/arg_mining_ukp/essay172.png\n",
      "saved png to /home/yochay/arg_mining_ukp/essay38.png\n",
      "saved png to /home/yochay/arg_mining_ukp/essay129.png\n",
      "saved png to /home/yochay/arg_mining_ukp/essay177.png\n",
      "saved png to /home/yochay/arg_mining_ukp/essay388.png\n",
      "saved png to /home/yochay/arg_mining_ukp/essay229.png\n",
      "saved png to /home/yochay/arg_mining_ukp/essay242.png\n",
      "saved png to /home/yochay/arg_mining_ukp/essay23.png\n",
      "saved png to /home/yochay/arg_mining_ukp/essay231.png\n",
      "saved png to /home/yochay/arg_mining_ukp/essay68.png\n",
      "saved png to /home/yochay/arg_mining_ukp/essay273.png\n",
      "saved png to /home/yochay/arg_mining_ukp/essay250.png\n",
      "saved png to /home/yochay/arg_mining_ukp/essay122.png\n",
      "saved png to /home/yochay/arg_mining_ukp/essay324.png\n",
      "saved png to /home/yochay/arg_mining_ukp/essay291.png\n",
      "saved png to /home/yochay/arg_mining_ukp/essay175.png\n",
      "saved png to /home/yochay/arg_mining_ukp/essay70.png\n",
      "saved png to /home/yochay/arg_mining_ukp/essay130.png\n",
      "saved png to /home/yochay/arg_mining_ukp/essay203.png\n",
      "saved png to /home/yochay/arg_mining_ukp/essay251.png\n",
      "saved png to /home/yochay/arg_mining_ukp/essay145.png\n"
     ]
    }
   ],
   "source": [
    "base = '/home/yochay/arg_mining_ukp/data/ArgumentAnnotatedEssays-2.0/ArgumentAnnotatedEssays-2.0/brat-project-final/brat-project-final'\n",
    "annotated = [base + \"/\" + fn for fn in os.listdir(base) if fn[-3:] == \"ann\" ]\n",
    "for fn in annotated:\n",
    "    with open(fn,'rt') as f:\n",
    "        done = False\n",
    "        for line in f:\n",
    "            if done:\n",
    "                break\n",
    "            delimited = line.split('\\t')\n",
    "            if line[0] == 'R' and delimited[1].split()[0] != \"supports\":\n",
    "                argDoc = ArgDoc(fn[:-4])\n",
    "                visualizeUKPArgDoc(argDoc)\n",
    "                done = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'supports Arg1:T5 Arg2:T11'"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delimited[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((251, 338),\n",
       " (445, 511),\n",
       " (513, 667),\n",
       " (669, 788),\n",
       " (790, 844),\n",
       " (857, 915),\n",
       " (938, 984),\n",
       " (986, 1157),\n",
       " (1166, 1246),\n",
       " (1248, 1355),\n",
       " (1365, 1426),\n",
       " (1443, 1531),\n",
       " (1533, 1584),\n",
       " (1586, 1643),\n",
       " (1645, 1713),\n",
       " (1726, 1783))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arg_doc_ex.prop_offsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(251, 338)\tMajorClaim\n",
      "(445, 511)\tPremise\n",
      "(513, 667)\tPremise\n",
      "(669, 788)\tPremise\n",
      "(790, 844)\tPremise\n",
      "(857, 915)\tClaim\n",
      "(938, 984)\tClaim\n",
      "(986, 1157)\tPremise\n",
      "(1166, 1246)\tPremise\n",
      "(1248, 1355)\tPremise\n",
      "(1365, 1426)\tPremise\n",
      "(1443, 1531)\tMajorClaim\n",
      "(1533, 1584)\tPremise\n",
      "(1586, 1643)\tPremise\n",
      "(1645, 1713)\tPremise\n",
      "(1726, 1783)\tClaim\n"
     ]
    }
   ],
   "source": [
    "[print(\"{}\\t{}\".format(arg_doc_ex.prop_offsets[i],arg_doc_ex.prop_labels[i])) for i in range(len(arg_doc_ex.prop_offsets))];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.parse import CoreNLPParser\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate new proposition offsets w/o spaces\n",
    "def calc_no_spaces_indices(arg_doc:ArgDoc)->[(int,int)]:\n",
    "    old_indices = arg_doc.prop_offsets\n",
    "    text = arg_doc.text\n",
    "    new_offsets = []\n",
    "    for (beg,end) in old_indices:\n",
    "        new_beg = len(text[:beg].replace(\" \",\"\"))\n",
    "        new_end = new_beg + len(text[beg:end].replace(\" \",\"\"))\n",
    "        new_offsets.append(new_beg, new_end)\n",
    "    return new_offsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The popularity of news media\n",
      "====================\n",
      "Original Sentence:\n",
      "The popularity of news media\n",
      "The\tDT\n",
      "popularity\tNN\n",
      "of\tIN\n",
      "news\tNN\n",
      "media\tNNS\n",
      "====================\n",
      "\n",
      "====================\n",
      "Nowadays news media have become more and more popular. Many people consider that the drawbacks of this phenomenon outweigh its merits. However, it is possible that this idea is not completely true. It is widely seen that news media not only brings people entertainment, but also polishes up people' knowledge. Therefore, it seems not unreasonable to suggest that this is a positive development.\n",
      "====================\n",
      "Original Sentence:\n",
      "Nowadays news media have become more and more popular.\n",
      "Nowadays\tRB\n",
      "news\tNN\n",
      "media\tNNS\n",
      "have\tVBP\n",
      "become\tVBN\n",
      "more\tRBR\n",
      "and\tCC\n",
      "more\tRBR\n",
      "popular\tJJ\n",
      ".\t.\n",
      "====================\n",
      "Original Sentence:\n",
      "Many people consider that the drawbacks of this phenomenon outweigh its merits.\n",
      "Many\tJJ\n",
      "people\tNNS\n",
      "consider\tVBP\n",
      "that\tIN\n",
      "the\tDT\n",
      "drawbacks\tNNS\n",
      "of\tIN\n",
      "this\tDT\n",
      "phenomenon\tNN\n",
      "outweigh\tVBP\n",
      "its\tPRP$\n",
      "merits\tNNS\n",
      ".\t.\n",
      "====================\n",
      "Original Sentence:\n",
      "However, it is possible that this idea is not completely true.\n",
      "However\tRB\n",
      ",\t,\n",
      "it\tPRP\n",
      "is\tVBZ\n",
      "possible\tJJ\n",
      "that\tIN\n",
      "this\tDT\n",
      "idea\tNN\n",
      "is\tVBZ\n",
      "not\tRB\n",
      "completely\tRB\n",
      "true\tJJ\n",
      ".\t.\n",
      "====================\n",
      "Original Sentence:\n",
      "It is widely seen that news media not only brings people entertainment, but also polishes up people' knowledge.\n",
      "It\tPRP\n",
      "is\tVBZ\n",
      "widely\tRB\n",
      "seen\tVBN\n",
      "that\tIN\n",
      "news\tNN\n",
      "media\tNNS\n",
      "not\tRB\n",
      "only\tRB\n",
      "brings\tVBZ\n",
      "people\tNNS\n",
      "entertainment\tNN\n",
      ",\t,\n",
      "but\tCC\n",
      "also\tRB\n",
      "polishes\tVBZ\n",
      "up\tRP\n",
      "people\tNNS\n",
      "'\tPOS\n",
      "knowledge\tNN\n",
      ".\t.\n",
      "====================\n",
      "Original Sentence:\n",
      "Therefore, it seems not unreasonable to suggest that this is a positive development.\n",
      "Therefore\tRB\n",
      ",\t,\n",
      "it\tPRP\n",
      "seems\tVBZ\n",
      "not\tRB\n",
      "unreasonable\tJJ\n",
      "to\tTO\n",
      "suggest\tVB\n",
      "that\tIN\n",
      "this\tDT\n",
      "is\tVBZ\n",
      "a\tDT\n",
      "positive\tJJ\n",
      "development\tNN\n",
      ".\t.\n",
      "====================\n",
      "First and foremost, today entertainment is easier for people to enjoy than ever before. The fact is that such news media as televisions, radios, or newspapers are bringing many news on entertainment sector from all over the world to everybody. People can sit in front of their televisions and watch whatever they want, from musical concerts to live football match. This is a convenient way of relaxation after hard work. Obviously, the popularity of news media is beneficial to human beings.\n",
      "====================\n",
      "Original Sentence:\n",
      "First and foremost, today entertainment is easier for people to enjoy than ever before.\n",
      "First\tNNP\n",
      "and\tCC\n",
      "foremost\tJJ\n",
      ",\t,\n",
      "today\tNN\n",
      "entertainment\tNN\n",
      "is\tVBZ\n",
      "easier\tJJR\n",
      "for\tIN\n",
      "people\tNNS\n",
      "to\tTO\n",
      "enjoy\tVB\n",
      "than\tIN\n",
      "ever\tRB\n",
      "before\tRB\n",
      ".\t.\n",
      "====================\n",
      "Original Sentence:\n",
      "The fact is that such news media as televisions, radios, or newspapers are bringing many news on entertainment sector from all over the world to everybody.\n",
      "The\tDT\n",
      "fact\tNN\n",
      "is\tVBZ\n",
      "that\tIN\n",
      "such\tJJ\n",
      "news\tNN\n",
      "media\tNNS\n",
      "as\tIN\n",
      "televisions\tNNS\n",
      ",\t,\n",
      "radios\tNNS\n",
      ",\t,\n",
      "or\tCC\n",
      "newspapers\tNNS\n",
      "are\tVBP\n",
      "bringing\tVBG\n",
      "many\tJJ\n",
      "news\tNN\n",
      "on\tIN\n",
      "entertainment\tNN\n",
      "sector\tNN\n",
      "from\tIN\n",
      "all\tDT\n",
      "over\tIN\n",
      "the\tDT\n",
      "world\tNN\n",
      "to\tTO\n",
      "everybody\tNN\n",
      ".\t.\n",
      "====================\n",
      "Original Sentence:\n",
      "People can sit in front of their televisions and watch whatever they want, from musical concerts to live football match.\n",
      "People\tNNS\n",
      "can\tMD\n",
      "sit\tVB\n",
      "in\tIN\n",
      "front\tNN\n",
      "of\tIN\n",
      "their\tPRP$\n",
      "televisions\tNNS\n",
      "and\tCC\n",
      "watch\tVB\n",
      "whatever\tWDT\n",
      "they\tPRP\n",
      "want\tVBP\n",
      ",\t,\n",
      "from\tIN\n",
      "musical\tJJ\n",
      "concerts\tNNS\n",
      "to\tTO\n",
      "live\tVB\n",
      "football\tNN\n",
      "match\tNN\n",
      ".\t.\n",
      "====================\n",
      "Original Sentence:\n",
      "This is a convenient way of relaxation after hard work.\n",
      "This\tDT\n",
      "is\tVBZ\n",
      "a\tDT\n",
      "convenient\tJJ\n",
      "way\tNN\n",
      "of\tIN\n",
      "relaxation\tNN\n",
      "after\tIN\n",
      "hard\tJJ\n",
      "work\tNN\n",
      ".\t.\n",
      "====================\n",
      "Original Sentence:\n",
      "Obviously, the popularity of news media is beneficial to human beings.\n",
      "Obviously\tRB\n",
      ",\t,\n",
      "the\tDT\n",
      "popularity\tNN\n",
      "of\tIN\n",
      "news\tNN\n",
      "media\tNNS\n",
      "is\tVBZ\n",
      "beneficial\tJJ\n",
      "to\tTO\n",
      "human\tJJ\n",
      "beings\tNNS\n",
      ".\t.\n",
      "====================\n",
      "In addition to this, news media help people broaden their knowledge. It is clearly seen that mass media provide people with a variety of global news on all fields, including political situation, economic change, global warming, and so forth. Hence, there is a strong likelihood that today people are able to know the world better. This is particularly essential when the globalization process has been permeating into all areas of society. Indeed, the fast growth of news media is helpful for people worldwide.\n",
      "====================\n",
      "Original Sentence:\n",
      "In addition to this, news media help people broaden their knowledge.\n",
      "In\tIN\n",
      "addition\tNN\n",
      "to\tTO\n",
      "this\tDT\n",
      ",\t,\n",
      "news\tNN\n",
      "media\tNNS\n",
      "help\tVBP\n",
      "people\tNNS\n",
      "broaden\tVB\n",
      "their\tPRP$\n",
      "knowledge\tNN\n",
      ".\t.\n",
      "====================\n",
      "Original Sentence:\n",
      "It is clearly seen that mass media provide people with a variety of global news on all fields, including political situation, economic change, global warming, and so forth.\n",
      "It\tPRP\n",
      "is\tVBZ\n",
      "clearly\tRB\n",
      "seen\tVBN\n",
      "that\tIN\n",
      "mass\tNN\n",
      "media\tNNS\n",
      "provide\tVBP\n",
      "people\tNNS\n",
      "with\tIN\n",
      "a\tDT\n",
      "variety\tNN\n",
      "of\tIN\n",
      "global\tJJ\n",
      "news\tNN\n",
      "on\tIN\n",
      "all\tDT\n",
      "fields\tNNS\n",
      ",\t,\n",
      "including\tVBG\n",
      "political\tJJ\n",
      "situation\tNN\n",
      ",\t,\n",
      "economic\tJJ\n",
      "change\tNN\n",
      ",\t,\n",
      "global\tJJ\n",
      "warming\tNN\n",
      ",\t,\n",
      "and\tCC\n",
      "so\tRB\n",
      "forth\tRB\n",
      ".\t.\n",
      "====================\n",
      "Original Sentence:\n",
      "Hence, there is a strong likelihood that today people are able to know the world better.\n",
      "Hence\tRB\n",
      ",\t,\n",
      "there\tEX\n",
      "is\tVBZ\n",
      "a\tDT\n",
      "strong\tJJ\n",
      "likelihood\tNN\n",
      "that\tWDT\n",
      "today\tNN\n",
      "people\tNNS\n",
      "are\tVBP\n",
      "able\tJJ\n",
      "to\tTO\n",
      "know\tVB\n",
      "the\tDT\n",
      "world\tNN\n",
      "better\tRBR\n",
      ".\t.\n",
      "====================\n",
      "Original Sentence:\n",
      "This is particularly essential when the globalization process has been permeating into all areas of society.\n",
      "This\tDT\n",
      "is\tVBZ\n",
      "particularly\tRB\n",
      "essential\tJJ\n",
      "when\tWRB\n",
      "the\tDT\n",
      "globalization\tNN\n",
      "process\tNN\n",
      "has\tVBZ\n",
      "been\tVBN\n",
      "permeating\tVBG\n",
      "into\tIN\n",
      "all\tDT\n",
      "areas\tNNS\n",
      "of\tIN\n",
      "society\tNN\n",
      ".\t.\n",
      "====================\n",
      "Original Sentence:\n",
      "Indeed, the fast growth of news media is helpful for people worldwide.\n",
      "Indeed\tRB\n",
      ",\t,\n",
      "the\tDT\n",
      "fast\tJJ\n",
      "growth\tNN\n",
      "of\tIN\n",
      "news\tNN\n",
      "media\tNNS\n",
      "is\tVBZ\n",
      "helpful\tJJ\n",
      "for\tIN\n",
      "people\tNNS\n",
      "worldwide\tRB\n",
      ".\t.\n",
      "====================\n",
      "In conclusion, the development of news media has been making great inroads into the lives of all people. It relaxes people with plentiful entertainment news. It enriches people's knowledge with worldwide information. It also positively affects the way people live in many other aspects. Therefore, the progress of news media is an advantageous development.\n",
      "====================\n",
      "Original Sentence:\n",
      "In conclusion, the development of news media has been making great inroads into the lives of all people.\n",
      "In\tIN\n",
      "conclusion\tNN\n",
      ",\t,\n",
      "the\tDT\n",
      "development\tNN\n",
      "of\tIN\n",
      "news\tNN\n",
      "media\tNNS\n",
      "has\tVBZ\n",
      "been\tVBN\n",
      "making\tVBG\n",
      "great\tJJ\n",
      "inroads\tNNS\n",
      "into\tIN\n",
      "the\tDT\n",
      "lives\tNNS\n",
      "of\tIN\n",
      "all\tDT\n",
      "people\tNNS\n",
      ".\t.\n",
      "====================\n",
      "Original Sentence:\n",
      "It relaxes people with plentiful entertainment news.\n",
      "It\tPRP\n",
      "relaxes\tVBZ\n",
      "people\tNNS\n",
      "with\tIN\n",
      "plentiful\tJJ\n",
      "entertainment\tNN\n",
      "news\tNN\n",
      ".\t.\n",
      "====================\n",
      "Original Sentence:\n",
      "It enriches people's knowledge with worldwide information.\n",
      "It\tPRP\n",
      "enriches\tVBZ\n",
      "people\tNNS\n",
      "'s\tPOS\n",
      "knowledge\tNN\n",
      "with\tIN\n",
      "worldwide\tJJ\n",
      "information\tNN\n",
      ".\t.\n",
      "====================\n",
      "Original Sentence:\n",
      "It also positively affects the way people live in many other aspects.\n",
      "It\tPRP\n",
      "also\tRB\n",
      "positively\tRB\n",
      "affects\tVBZ\n",
      "the\tDT\n",
      "way\tNN\n",
      "people\tNNS\n",
      "live\tVBP\n",
      "in\tIN\n",
      "many\tJJ\n",
      "other\tJJ\n",
      "aspects\tNNS\n",
      ".\t.\n",
      "====================\n",
      "Original Sentence:\n",
      "Therefore, the progress of news media is an advantageous development.\n",
      "Therefore\tRB\n",
      ",\t,\n",
      "the\tDT\n",
      "progress\tNN\n",
      "of\tIN\n",
      "news\tNN\n",
      "media\tNNS\n",
      "is\tVBZ\n",
      "an\tDT\n",
      "advantageous\tJJ\n",
      "development\tNN\n",
      ".\t.\n",
      "====================\n",
      "\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "arg_doc = arg_doc_ex\n",
    "text = arg_doc_ex.text\n",
    "pos_tagger = CoreNLPParser(url='http://localhost:9000', tagtype='pos')\n",
    "output_file = '/home/yochay/arg_mining_proj/data/095ex.preprocessed'\n",
    "\n",
    "EMPTY_SIGN = \"~\"\n",
    "\n",
    "def pre_process_ukp_essay(base_path, pos_tagger):\n",
    "    arg_doc = ArgDoc(base_path)\n",
    "    output_file = base_path + \".tsv\"\n",
    "    with open(output_file,'wt',encoding='utf8') as out_f:\n",
    "        i_no_space = 0\n",
    "        paragraphs = []\n",
    "        # list of sentences (list of list of tuples representing tokens and POSs)\n",
    "        for paragraph in text.split('\\n'):\n",
    "            # token,POS-tag,no_space_index tuple\n",
    "            tagged_sentences = []\n",
    "            # skip empty lines (usually seperated from essay title)\n",
    "            if len(paragraph) == 0:\n",
    "                continue\n",
    "            # use nltk sentence tokenizer (PunktSentenceTokenizer)\n",
    "            sentences = sent_tokenize(paragraph)\n",
    "            for sent in sentences:\n",
    "                # use Stanford's CoreNLP for POS tagging sentence by sentence\n",
    "                pos_tagged_sent = pos_tagger.tag(sent.split())\n",
    "                tok_pos_noSpaceIndex_sent = []\n",
    "                for tok, pos in tagged_sent:\n",
    "                    tok_pos_noSpaceIndex_sent.append(tok,pos,i_no_space)\n",
    "                    i_no_space += len(tok)\n",
    "                tagged_sentences.append(tok_pos_noSpaceIndex_sent)\n",
    "            paragraphs.append(tagged_sentences)\n",
    "\n",
    "        # add appropriate AC tags by propositions\n",
    "        no_space_prop_offsets = calc_no_spaces_indices(arg_doc)\n",
    "\n",
    "        for i_paragraph in range(len(paragraphs)):\n",
    "            out_f.write(\"# paragraph {}\\n\".format(i_paragraph))\n",
    "            for tagged_sentence in paragraphs(i_paragraph):\n",
    "                out_f.write(\"# sent\\n\")\n",
    "                for tok,pos,i_no_space in tagged_sentence:\n",
    "                    # handle AC tagging where propositions apply\n",
    "                    # inefficient but written in haste for 0ne-time use ... TODO: improve later\n",
    "                    for i_prop in range(len(no_space_prop_offsets)):\n",
    "                        # if the current token is in proposition i_prop\n",
    "                        if (i_no_space => no_space_prop_offsets[i_prop][0] and < no_space_prop_offsets[i_prop][1]):\n",
    "                            # tag AC information as required (beginning(B) or middle(I) of proposition + AC type)\n",
    "                            ac_type = arg_doc.prop_lables[i_prop]\n",
    "                            ac_bio_tag = join(\"B-\" + ac_type) if i_no_space == no_space_prop_offsets[i_prop][0] else join(\"I-\" + ac_type)\n",
    "                            # tag relation information according to AC type ({AC index:supports\\attacks} for premise, For/Against for Claim, empty tab for MajorClaim)\n",
    "                            rel_tag = EMPTY_SIGN\n",
    "                            if (ac_type == \"Premise\"):\n",
    "                                # either it supports or attacks a claim\n",
    "                                support_prems, supported = zip(*arg_doc.supports)\n",
    "                                if i_prop in support_prems:\n",
    "                                    rel_tag = \"supports:{}\".format(supported[support_prems.index(i_prop)])\n",
    "                                else:\n",
    "                                    attack_prems, attacked = zip(*arg_doc.attacks)\n",
    "                                    rel_tag = \"attacks:{}\".format(attacked[attack_prems.index(i_prop)])\n",
    "                            elif (ac_type == \"Claim\"):\n",
    "                                # Claims only have For or Against relation type (they refer to the essay's major claims)\n",
    "                                rel_tag = \"{}:{}\".format(arg_doc.stances[i_prop],EMPTY_SIGN)\n",
    "\n",
    "                            f_out.write(\"\\t\".join(tok,pos,ac_bio_tag,i_prop,rel_tag))\n",
    "                        else:\n",
    "                            f_out.write(\"\\t\".join(tok,pos,EMPTY_SIGN,EMPTY_SIGN,EMPTY_SIGN))\n",
    "                        f_out.write(\"\\n\")\n",
    "                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{15: 'For', 5: 'For', 6: 'For'}"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arg_doc_ex.prop_stances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
