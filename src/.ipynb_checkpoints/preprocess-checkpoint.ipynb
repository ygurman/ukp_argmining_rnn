{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## preprocess the UKP Annotated Essays v2 DB\n",
    "# read .ann files and convert to CONLL style token-based Tab-delimited format (essay and/or paragraph level)\n",
    "# format: {INDEX}|{TOKEN}|{POS}|{AC-BIO}|{AC-IND}|{REL-TAG} where:\n",
    "#     {POS} - a Stanford's CoreNLP POS tagger output\n",
    "#     {AC-BIO} - Argument Component tag (standard B-I-O tags with Entity types of {Premise, Claim, MajorClaim})\n",
    "#     {AC-IND} - Argument Component index\n",
    "#     {REL-TAG} - Argument Relation tag of form \"{R-TYPE}:#\" (Type from {Support,Attack,For,Against}, # is the AC-IND of related AC)\n",
    "\n",
    "### NLTK's Stanfords CoreNLP wrapper - https://github.com/nltk/nltk/wiki/Stanford-CoreNLP-API-in-NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse data from \".ann\" UKP 2.0 files\n",
    "def readAnnotatedFile(ann_path:str) -> (dict, dict, dict, list, list):\n",
    "    propositions, prop_labels, prop_stances, supports, attacks = {}, {}, {}, [], []\n",
    "    with open(file=ann_path, mode='rt', encoding='utf8') as f:\n",
    "        for line in f:\n",
    "            delimited = line.split('\\t')\n",
    "            typ = delimited[0][0] # T == proposition , A = Stance, R = link\n",
    "            inner_index = int(delimited[0][1:])\n",
    "            data = delimited[1].split()\n",
    "            \n",
    "            if typ == 'T':\n",
    "                label = data[0] # prop lable (Premise, Cliam or MajorClaim)\n",
    "                start, end = int(data[1]), int(data[2]) # proposition offsets\n",
    "                propositions[inner_index] = (start, end) # represent propositions by it's index boundries\n",
    "                prop_labels[inner_index] = label\n",
    "                \n",
    "            elif typ == 'A':\n",
    "                _, target_index, stance_value = data # first Column in \"A\" lines is always \"Stance\", stance value in {For, Against}\n",
    "                prop_stances[int(target_index[1:])] = stance_value\n",
    "                \n",
    "            elif typ == 'R':\n",
    "                link_typ = data[0] # link type in {supports, attacks}\n",
    "                source, target = int(data[1][6:]), int(data[2][6:]) #get inner indices of related propositions (ex:Arg1:T4 Arg2:T3 -> source == 4 , target = 3)\n",
    "                link_list = supports if link_typ == 'supports' else attacks\n",
    "                link_list.append((source,target))\n",
    "    \n",
    "    return propositions, prop_labels, prop_stances, supports, attacks\n",
    "\n",
    "def getNewLineIndices(text:str) -> np.array:\n",
    "    i = 0 # assume first char always opens paragraph\n",
    "    paragraph_indices = []\n",
    "    while i != -1:\n",
    "        paragraph_indices.append(i)\n",
    "        i = text.find('\\n', i + 1)\n",
    "    \n",
    "    return np.array(paragraph_indices)\n",
    "\n",
    "class ArgDoc(object):\n",
    "    def __init__(self, base_path):\n",
    "        self.ess_id = int(base_path[-3:]) # essay id according to UKP naming convention\n",
    "        self._txt_path = base_path + \".txt\" # essay text file path\n",
    "        self._ann_path = base_path + \".ann\" # UKP annotated file path\n",
    "        # read document's text\n",
    "        with open(file=self._txt_path, mode='rt', encoding='utf8') as f:\n",
    "            self.text = f.read()\n",
    "        \n",
    "        # get essay's paragraph's indices (seperated with '\\n')\n",
    "        self.paragraph_offsets = getNewLineIndices(self.text)\n",
    "        \n",
    "        # read annotated data from file\n",
    "        propositions, prop_labels, prop_stances, supports, attacks = readAnnotatedFile(self._ann_path)\n",
    "        \n",
    "        # update proposition offsets, labels, stances and link types\n",
    "        inner_indices, self.prop_offsets = zip(*sorted(propositions.items(), key = lambda x: x[1])) # use the beginning index of propositions for sort\n",
    "       \n",
    "        # paragraph alignmnt of propositions (ordered by proposition's offsets)\n",
    "        self.prop_paragraphs = [np.searchsorted(self.paragraph_offsets, start) -1 for start, _ in self.prop_offsets]\n",
    "        \n",
    "        # invert indices for key management \n",
    "        new_indices = {k: v for v, k in enumerate(inner_indices)}\n",
    "        n_props = len(self.prop_offsets)\n",
    "        \n",
    "        # update fields with new inverted indices\n",
    "        self.prop_labels = [prop_labels[inner_indices[i]] for i in range(n_props)]\n",
    "        self.prop_stances = {new_indices[k]: v for k,v in prop_stances.items()}\n",
    "        self.supports = [(new_indices[src], new_indices[trg]) for src, trg in supports]\n",
    "        self.attacks = [(new_indices[src], new_indices[trg]) for src, trg in attacks]\n",
    "        self.links = self.supports + self.attacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the \"start_core_nlp_server.sh\" script before (sets a stanfoed's corenlp server in port 9000)\n",
    "from nltk.parse import CoreNLPParser\n",
    "from nltk.tokenize import sent_tokenize\n",
    "pos_tagger = CoreNLPParser(url='http://localhost:9000', tagtype='pos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMPTY_SIGN = \"~\"\n",
    "\n",
    "# calculate new proposition offsets w/o spaces\n",
    "def calc_no_spaces_indices(arg_doc:ArgDoc)->[(int,int)]:\n",
    "    old_indices = arg_doc.prop_offsets\n",
    "    text = arg_doc.text\n",
    "    new_offsets = []\n",
    "    for (beg,end) in old_indices:\n",
    "        new_beg = len(text[:beg].replace(\" \",\"\"))\n",
    "        new_end = new_beg + len(text[beg:end].replace(\" \",\"\"))\n",
    "        new_offsets.append((new_beg, new_end))\n",
    "    return new_offsets\n",
    "\n",
    "# read .ann and .txt of essay# and craete appropriate token-level conll-like file as mentioned above\n",
    "def pre_process_ukp_essay(base_path, pos_tagger = CoreNLPParser(url='http://localhost:9000', tagtype='pos')):\n",
    "    arg_doc = ArgDoc(base_path)\n",
    "    output_file = base_path + \".tsv\"\n",
    "    with open(output_file,'wt',encoding='utf8') as f_out:\n",
    "        i_no_space = 0\n",
    "        paragraphs = []\n",
    "        # list of sentences (list of list of tuples representing tokens and POSs)\n",
    "        for paragraph in arg_doc.text.split('\\n'):\n",
    "            # token,POS-tag,no_space_index tuple\n",
    "            tagged_sentences = []\n",
    "            # skip empty lines (usually seperated from essay title)\n",
    "            if len(paragraph) == 0:\n",
    "                continue\n",
    "            # use nltk sentence tokenizer (PunktSentenceTokenizer)\n",
    "            sentences = sent_tokenize(paragraph)\n",
    "            for sent in sentences:\n",
    "                # use Stanford's CoreNLP for POS tagging sentence by sentence\n",
    "                pos_tagged_sent = pos_tagger.tag(sent.split())\n",
    "                tok_pos_noSpaceIndex_sent = []\n",
    "                for tok, pos in pos_tagged_sent:\n",
    "                    tok_pos_noSpaceIndex_sent.append((tok,pos,i_no_space))\n",
    "                    i_no_space += len(tok)\n",
    "                tagged_sentences.append(tok_pos_noSpaceIndex_sent)\n",
    "            paragraphs.append(tagged_sentences)\n",
    "\n",
    "        # add appropriate AC tags by propositions\n",
    "        no_space_prop_offsets = calc_no_spaces_indices(arg_doc)\n",
    "\n",
    "        for i_paragraph in range(len(paragraphs)):\n",
    "            f_out.write(\"# paragraph {}\\n\".format(i_paragraph))\n",
    "            for tagged_sentence in paragraphs[i_paragraph]:\n",
    "                f_out.write(\"# sent\\n\")\n",
    "                for tok,pos,i_no_space in tagged_sentence:\n",
    "                    # handle AC tagging where propositions apply\n",
    "                    # inefficient but written in haste for 0ne-time use ... TODO: improve later\n",
    "                    for i_prop in range(len(no_space_prop_offsets)):\n",
    "                        # if the current token is in proposition i_prop\n",
    "                        if (i_no_space >= no_space_prop_offsets[i_prop][0] and i_no_space < no_space_prop_offsets[i_prop][1]):\n",
    "                            # tag AC information as required (beginning(B) or middle(I) of proposition + AC type)\n",
    "                            ac_type = arg_doc.prop_labels[i_prop]\n",
    "                            ac_bio_tag = \"B-\" + ac_type if i_no_space == no_space_prop_offsets[i_prop][0] else \"I-\" + ac_type\n",
    "                            # tag relation information according to AC type ({AC index:supports\\attacks} for premise, For/Against for Claim, empty tab for MajorClaim)\n",
    "                            rel_tag = EMPTY_SIGN\n",
    "                            if (ac_type == \"Premise\"):\n",
    "                                # either it supports or attacks a claim\n",
    "                                support_prems, supported = zip(*arg_doc.supports)\n",
    "                                if i_prop in support_prems:\n",
    "                                    rel_tag = \"supports:{}\".format(supported[support_prems.index(i_prop)])\n",
    "                                else:\n",
    "                                    attack_prems, attacked = zip(*arg_doc.attacks)\n",
    "                                    rel_tag = \"attacks:{}\".format(attacked[attack_prems.index(i_prop)])\n",
    "                            elif (ac_type == \"Claim\"):\n",
    "                                # Claims only have For or Against relation type (they refer to the essay's major claims)\n",
    "                                rel_tag = \"{}:{}\".format(arg_doc.prop_stances[i_prop],EMPTY_SIGN)\n",
    "\n",
    "                            f_out.write(\"\\t\".join((tok,pos,ac_bio_tag,str(i_prop),rel_tag)))\n",
    "                        else:\n",
    "                            f_out.write(\"\\t\".join((tok,pos,EMPTY_SIGN,EMPTY_SIGN,EMPTY_SIGN)))\n",
    "                        f_out.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate over all the essays and create according conll files\n",
    "data_path = os.path.join(os.getcwd(),\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "test_path = \"/home/yochay/arg_mining_proj/data/essay001\"\n",
    "pre_process_ukp_essay(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
